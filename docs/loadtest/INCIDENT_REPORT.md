# 장애 대응 보고서 - 주문 API 성능 저하 장애

**장애 일시**: 2025-12-30 14:00 ~ 14:35 (35분간)
**장애 등급**: P1 (Critical - 핵심 비즈니스 기능 중단)
**작성일**: 2025-12-30
**작성자**: 조솔하
**시스템**: E-commerce Core System

---

## 📋 현상

### 타임라인

| 시간 | 이벤트 | 담당자/시스템 |
|------|--------|--------------|
| 14:00 | 점심시간 주문 시작, 평소보다 응답 느림 감지 | 자동 모니터링 |
| 14:03 | 고객센터 문의 시작 ("주문 버튼 눌렀는데 계속 로딩만 돼요") | CS팀 |
| 14:05 | 고객 문의 급증 (분당 50건 → 200건) | CS팀 |
| 14:07 | **모니터링 알람 발생**: 주문 API p95 응답시간 30초 초과 | Grafana Alert |
| 14:08 | **장애 상황 인지** - CS팀 → 개발팀 긴급 전파 | CS팀 리더 |
| 14:10 | 장애 대응 시작, Slack #incident 채널 개설 | 백엔드 팀장 |
| 14:10 | 온콜 개발자 긴급 소집 (3명) | DevOps |
| 14:12 | 애플리케이션 로그 분석 시작 | 백엔드 개발자 A |
| 14:15 | APM 도구로 병목 구간 확인 중 | 백엔드 개발자 B |
| 14:18 | Redis Lock 대기 큐 확인 - 수천 건 대기 중 발견 | 백엔드 개발자 A |
| 14:20 | **원인 특정**: 전역 분산 락으로 인한 순차 처리 확인 | 백엔드 개발자 A |
| 14:22 | 긴급 핫픽스 코드 작성 시작 (전역 락 → 사용자별 락) | 백엔드 개발자 A, C |
| 14:25 | 핫픽스 코드 완성 및 로컬 테스트 | 백엔드 개발자 A |
| 14:27 | 긴급 코드 리뷰 (Tech Lead 승인) | Tech Lead |
| 14:28 | 스테이징 환경 긴급 배포 및 검증 | DevOps |
| 14:30 | **프로덕션 배포 시작** | DevOps |
| 14:32 | 배포 완료, TPS 회복 확인 (7 → 82) | 모니터링 시스템 |
| 14:33 | 응답시간 정상화 확인 (p95: 30초 → 130ms) | 백엔드 개발자 B |
| 14:35 | **장애 해소 선언** | 백엔드 팀장 |
| 14:37 | CS팀 공지: "주문 서비스 정상화" | CS팀 |
| 14:40 | 고객 대상 앱 푸시/이메일 발송 | 마케팅팀 |
| 15:00 | 장애 모니터링 지속 (재발 방지) | 백엔드 팀 전체 |

### 영향범위

**시스템 영향:**
- 장애 대상 API: `POST /api/orders` (주문/결제)
- 장애 기간: 35분간 (14:00 ~ 14:35)
- 관련 시스템:
  - 주문 서비스 ❌ (완전 마비)
  - 장바구니 서비스 ✅ (정상)
  - 쿠폰 서비스 ✅ (정상)
  - 결제 서비스 ✅ (정상, but 주문이 안 생성되어 사용 불가)

**사용자 영향:**
- 동시 접속자: 약 5,000명
- 주문 시도: 4,856건
- 실패한 주문: 4,176건 (85.9%)
- 성공한 주문: 680건 (14.1%, 초기 트래픽 낮을 때만 성공)
- 타임아웃 경험 사용자: 약 4,000명

**지역별 영향:**
- 전국 전체 사용자 영향
- 특히 점심시간 주문 집중 지역 (서울, 경기) 타격 큺

### 고객영향도 (비즈니스 임팩트)

**즉각적 영향:**
1. **매출 손실**
   - 실패한 주문: 4,176건
   - 평균 주문 금액: 약 6,000원
   - 추정 매출 손실: **약 2,500만원**

2. **고객 경험 악화**
   - 고객센터 문의: 340건 (평소 대비 8배)
   - 앱 리뷰 악화: 별점 1점 리뷰 52건 급증
   - SNS 부정 멘션: 트위터/인스타 127건
   - 환불 요청: 18건 (중복 결제로 오인한 경우)

3. **운영 비용 증가**
   - 긴급 대응 인력 투입: 개발 3명, DevOps 2명, CS 10명
   - 긴급 배포 비용
   - CS 추가 근무 비용

**장기적 영향:**
1. **브랜드 신뢰도 하락**
   - "점심시간에 주문이 안 되는 앱" 이미지
   - 경쟁사 이탈 가능성 증가

2. **재구매율 영향**
   - 해당 시간대 이탈 고객 약 500명 추정
   - 향후 2주간 재구매율 모니터링 필요

3. **마케팅 효과 감소**
   - 진행 중이던 "점심 특가" 이벤트 효과 반감
   - 이벤트 예산 대비 ROI 60% 감소

---

## 🔧 조치 내용

### 장애 원인

#### 직접 원인
**전역 분산 락(Global Distributed Lock)으로 인한 병목 발생**

```java
// 문제가 된 코드 (CreateOrderUseCase.java:45)
@DistributedLock(key = "'order:global'")  // ❌ 모든 사용자가 이 락을 대기
public OrderResponse execute(CreateOrderRequest request) {
    return createOrderService.create(request);
}
```

**문제점:**
- 모든 주문 요청이 단일 Redis Lock `LOCK:order:global`을 획득해야만 처리 가능
- Lock은 동시에 1개만 획득 가능 → 나머지는 모두 대기
- 100명이 동시 주문 시도 → 실제로는 순차 처리 (동시성 = 1)
- 평균 주문 처리 시간 130ms × 순차 처리 = TPS 7.7

**측정된 성능 지표:**
```
장애 상황 (14:00 ~ 14:35):
- TPS: 7.54 req/s  (정상 시 목표: 50 이상)
- 평균 응답시간: 8,200ms  (정상: 150ms 이하)
- p(95) 응답시간: 30,000ms  (30초!)
- p(99) 응답시간: 60,000ms+ (타임아웃)
- 성공률: 14.1%
- 에러율: 85.9% (대부분 타임아웃)
```

#### 근본 원인

**기술적 원인:**

1. **Lock Granularity 설계 오류**
   - 최초 설계 시 "모든 주문은 전역적으로 순서를 보장해야 한다"는 잘못된 전제
   - 실제로는 사용자별로 독립적인 Lock만 필요함
   - 서로 다른 사용자의 주문은 동시 처리 가능함을 간과

2. **동시성 제어 전략 검토 부족**
   - 분산 락 도입 시 Lock의 범위(Scope)에 대한 고민 부족
   - "동시성 제어 = 전역 Lock"이라는 단순한 사고
   - Pessimistic Lock(DB)과 Distributed Lock(Redis)의 역할 혼동

3. **부하 테스트 미실시**
   - 개발 환경에서는 1~2명만 테스트하여 문제 미발견
   - 통합 테스트에서 동시성 테스트 누락
   - 실제 트래픽 패턴 시뮬레이션 부재

**프로세스 원인:**

1. **코드 리뷰 단계에서 미발견**
   - Lock 범위에 대한 리뷰 질문 없음
   - "동시성 제어가 되어 있으니 OK" 수준의 표면적 리뷰

2. **성능 테스트 프로세스 부재**
   - 배포 전 필수 성능 검증 절차 없음
   - QA 팀의 부하 테스트 체크리스트 미흡
   - "기능이 동작하면 배포" 문화

3. **모니터링 알람 설정 미흡**
   - TPS 하락에 대한 알람 없음
   - p(95) 응답시간 알람 임계값이 너무 높음 (30초)
   - 5분 동안 장애를 자동 감지하지 못함

### 해소 타임라인

```
14:10 - 장애 대응 시작
   ↓
14:12 - 로그 분석 (ApplicationLog, AccessLog)
   ├─ "OrderController: Waiting for lock" 반복 발견
   └─ 평균 응답시간 8초 확인
   ↓
14:15 - APM 트레이싱 확인
   ├─ 병목: DistributedLock.tryLock() 7.8초 대기
   └─ 실제 비즈니스 로직은 200ms 소요
   ↓
14:18 - Redis 모니터링
   ├─ redis-cli> KEYS LOCK:order:*
   ├─ LOCK:order:global 발견
   ├─ redis-cli> GET LOCK:order:global  → 특정 스레드가 점유 중
   └─ redis-cli> LLEN order:waiting_queue  → 4,856 (!)
   ↓
14:20 - 원인 특정 완료
   ├─ 전역 락으로 인한 병목 확인
   ├─ Lock Key 분석: "'order:global'" 하드코딩 발견
   └─ 해결 방안 수립: 사용자별 Lock으로 변경
   ↓
14:22 - 긴급 핫픽스 작성
   ├─ Lock Key 변경: 'order:global' → 'order:user:{userId}'
   ├─ 추가 최적화: Connection Pool 확대, 인덱스 추가
   └─ 단위 테스트 작성 및 검증
   ↓
14:25 - 로컬 부하 테스트 (k6)
   ├─ VU 100, 30초 테스트
   ├─ TPS: 7.5 → 82 확인 ✅
   └─ 중복 주문 방지 동작 확인 ✅
   ↓
14:27 - 긴급 코드 리뷰
   ├─ Tech Lead 승인 (2분 리뷰)
   └─ "Lock 범위 변경, 동시성 제어 유지 확인"
   ↓
14:28 - 스테이징 배포 및 검증
   ├─ 배포 완료
   ├─ Smoke Test 통과
   └─ 1분간 모니터링 → 정상
   ↓
14:30 - 프로덕션 배포
   ├─ Blue-Green 배포 진행
   ├─ New Version 트래픽 10% → 50% → 100% 전환
   └─ 총 배포 시간: 2분
   ↓
14:32 - 성능 회복 확인
   ├─ TPS: 7.5 → 82 회복 ✅
   ├─ p(95) 응답시간: 30초 → 130ms ✅
   └─ 에러율: 85% → 8% ✅
   ↓
14:35 - 장애 해소 선언
```

### 실제 단기 대응책

#### 1. 전역 락 → 사용자별 락 변경 (핵심 수정)

```java
// Before (문제 코드)
@DistributedLock(key = "'order:global'")
public OrderResponse execute(CreateOrderRequest request) {
    return createOrderService.create(request);
}

// After (긴급 핫픽스)
@DistributedLock(key = "'order:user:' + #request.userId")
public OrderResponse execute(CreateOrderRequest request) {
    return createOrderService.create(request);
}
```

**변경 이유:**
- 서로 다른 사용자의 주문은 동시 처리 가능
- 동일 사용자의 중복 주문만 방지하면 됨
- 동시 처리 능력: 1명 → 5,000명 (사용자 수만큼)

**안전성 검증:**
```java
// DB 레벨 재고 검증 (이미 구현되어 있음)
@Lock(LockModeType.PESSIMISTIC_WRITE)
@Query("SELECT s FROM Stock s WHERE s.productId = :productId")
Stock findByProductIdWithLock(@Param("productId") String productId);

// 잔액 검증 (이미 구현되어 있음)
@Lock(LockModeType.PESSIMISTIC_WRITE)
@Query("SELECT u FROM User u WHERE u.id = :userId")
User findByIdWithLock(@Param("userId") String userId);
```

**Defense in Depth 전략:**
- Redis Lock (1차): 동일 사용자 중복 방지
- DB Pessimistic Lock (2차): 재고/잔액 정합성 보장
- Unique Constraint (3차): DB 레벨 중복 방지

#### 2. Connection Pool 확대

```yaml
# Before (application.yml)
spring:
  datasource:
    hikari:
      maximum-pool-size: 10

# After
spring:
  datasource:
    hikari:
      maximum-pool-size: 100
      minimum-idle: 20
      connection-timeout: 30000
      idle-timeout: 600000
      max-lifetime: 1800000
```

**효과:**
- 동시 접속 100명 환경 대응
- Connection 대기 시간: 50ms → 0ms

#### 3. 인덱스 추가

```java
// Before
@Table(name = "cart_items")
public class CartItem { ... }

// After
@Table(name = "cart_items", indexes = {
    @Index(name = "idx_cart_id", columnList = "cart_id")
})
public class CartItem { ... }
```

**효과:**
- 장바구니 조회: 60ms → 2ms (30배 개선)
- Full Table Scan 제거

### 후속 대응 계획

#### 즉시 조치 (배포 후 1시간 내)

1. **집중 모니터링** ✅ 완료
   - 15분 단위 TPS, 응답시간, 에러율 확인
   - 재발 없음 확인

2. **고객 공지** ✅ 완료
   - 앱 푸시: "주문 서비스 정상화되었습니다"
   - 이메일: 장애 사과 및 쿠폰 지급 안내

3. **피해 보상** ✅ 진행 중
   - 타임아웃 경험 고객 대상 3,000원 쿠폰 지급
   - 재주문 고객 배송비 무료

#### 24시간 내

1. **긴급 회고 미팅** (당일 18:00)
   - 참석: 백엔드팀, DevOps, QA, Tech Lead
   - 안건: 원인 분석, 재발 방지 대책

2. **전체 API Lock 전략 점검**
   - 다른 API에 동일 문제 있는지 긴급 감사
   - 쿠폰 발급 API, 결제 API 등 검토

3. **모니터링 알람 개선**
   - TPS 급감 알람 추가 (기준선 대비 50% 하락 시)
   - 응답시간 알람 임계값 조정 (p95 > 2초)

---

## 🔍 분석

### 5-Whys 근본 원인 분석

#### Why #1: 왜 장애가 발생했는가?
**→ 주문 API의 TPS가 7.54로 급감하여 대부분의 요청이 실패했기 때문**

**증거:**
- 모니터링 로그: TPS 평소 85 → 7.5로 급감
- 85.9%의 요청이 타임아웃 또는 실패
- 고객 문의 폭증 (평소 대비 8배)

---

#### Why #2: 왜 TPS가 7.54로 급감했는가?
**→ 전역 분산 락으로 인해 모든 주문이 순차 처리되었기 때문**

**증거:**
```bash
# Redis 모니터링 결과
redis-cli> LLEN order:waiting_queue
4856  # 4,856개 요청이 Lock 대기 중

redis-cli> KEYS LOCK:order:*
1) "LOCK:order:global"  # 단일 Lock만 존재
```

**분석:**
- 100명이 동시 주문 시도 → 실제로는 1명씩 순차 처리
- 주문 처리 시간 130ms × 순차 = 초당 7.7건만 처리 가능
- 이론적 최대 TPS = 1000ms / 130ms = 7.69

---

#### Why #3: 왜 전역 락을 사용했는가?
**→ Lock Granularity를 고려하지 않고 "모든 주문은 순서 보장이 필요하다"고 잘못 판단했기 때문**

**코드 리뷰 기록 확인:**
```
2025-12-20 PR #234 코드 리뷰
Reviewer: "동시성 제어가 되어 있나요?"
Author: "네, DistributedLock으로 처리했습니다."
Reviewer: "LGTM"  ← Lock 범위에 대한 질문 없음
```

**개발자 인터뷰:**
- "재고 차감의 정합성을 보장하려면 전역 Lock이 필요하다고 생각했습니다."
- "사용자별 Lock도 고려했지만, 더 안전하게 전역 Lock을 선택했습니다."
- **잘못된 판단**: 안전성과 성능의 트레이드오프를 고려하지 않음

**실제로는:**
- 재고 차감은 DB의 Pessimistic Lock으로 보장됨
- 사용자별 Lock만으로 중복 주문 방지 가능
- 전역 Lock은 불필요하고 해로움

---

#### Why #4: 왜 Lock Granularity를 고려하지 않았는가?
**→ 실제 트래픽 환경에서 부하 테스트를 하지 않았기 때문**

**배포 전 테스트 기록:**
```
테스트 케이스:
1. 단일 사용자 주문 생성 - PASS
2. 재고 부족 시 실패 - PASS
3. 잔액 부족 시 실패 - PASS
4. 쿠폰 적용 - PASS

부하 테스트: 없음 ❌
```

**발견된 사실:**
- 개발 환경에서 1~2명만 테스트 → 문제 미발견
- 통합 테스트에 동시성 시나리오 누락
- "기능이 동작하면 OK" 문화

**만약 부하 테스트를 했다면:**
- VU 50만 되어도 TPS 7.5 발견 가능
- 배포 전 문제 수정 가능
- 장애 예방 가능

---

#### Why #5: 왜 부하 테스트를 하지 않았는가?
**→ 배포 프로세스에 성능 검증이 필수가 아니었기 때문**

**현재 배포 프로세스:**
```
1. 개발 완료
2. 단위 테스트 작성 (필수)
3. 통합 테스트 (필수)
4. 코드 리뷰 (필수)
5. QA 검증 (필수)
6. 스테이징 배포
7. 프로덕션 배포

부하 테스트: 선택 사항 (실제로는 거의 안 함) ❌
```

**문제점:**
- 성능 테스트가 "Nice to Have" 수준
- QA 팀의 부하 테스트 체크리스트 미흡
- 부하 테스트 도구 및 환경 부재
- "장애 나봐야 중요성을 안다" 문화

---

### 근본 원인 요약

| 레벨 | 원인 | 유형 |
|------|------|------|
| Why #1 | TPS 7.54로 급감 | 증상 |
| Why #2 | 전역 락으로 순차 처리 | 기술적 원인 |
| Why #3 | Lock Granularity 미고려 | 설계 오류 |
| Why #4 | 부하 테스트 미실시 | 프로세스 부재 |
| Why #5 | **성능 검증이 필수가 아님** | **근본 원인** |

**근본 원인**: 배포 프로세스에 성능 검증이 필수 절차로 자리잡지 못함

---

## 💡 대응 방안

### Short-term (즉시 적용 완료)

#### ✅ 1. 전역 락 → 사용자별 락 변경 (완료)
```java
@DistributedLock(key = "'order:user:' + #request.userId")
```
- **효과**: TPS 7.54 → 81.68 (10.8배 향상)
- **배포일**: 2025-12-30 14:30

#### ✅ 2. Connection Pool 확대 (완료)
```yaml
maximum-pool-size: 10 → 100
```
- **효과**: Connection 대기 시간 50ms → 0ms
- **배포일**: 2025-12-30 14:30

#### ✅ 3. 인덱스 추가 (완료)
```java
@Index(name = "idx_cart_id", columnList = "cart_id")
```
- **효과**: 장바구니 조회 60ms → 2ms
- **배포일**: 2025-12-30 14:30

#### ✅ 4. 고객 보상 (진행 중)
- 타임아웃 경험 고객 4,000명에게 3,000원 쿠폰 지급
- 예상 비용: 1,200만원 (매출 손실 2,500만원의 48%)

---

### Mid-term (1주 내)

#### 🔄 1. 전체 API Lock 전략 감사 (예정: 2026-01-03)
**담당**: 백엔드팀 전체

**점검 대상:**
- [x] 주문 API: 사용자별 락으로 수정 완료 ✅
- [ ] 쿠폰 발급 API: 전역 락 사용 중 → 검토 필요
- [ ] 결제 API: 사용자별 락 사용 중 → 정상
- [ ] 포인트 충전 API: DB Lock만 사용 → 정상

**점검 기준:**
- Lock 범위가 필요 이상으로 넓지 않은가?
- 서로 다른 사용자/리소스가 동시 처리 가능한가?
- DB Lock과 분산 Lock의 역할이 명확한가?

#### 🔄 2. 부하 테스트 필수화 (예정: 2026-01-06)
**담당**: QA팀 + 백엔드팀

**계획:**
- [ ] k6 부하 테스트 스크립트 작성 (주요 API 10개)
- [ ] CI/CD 파이프라인에 통합 (PR 생성 시 자동 실행)
- [ ] 성능 기준선 수립 (API별 목표 TPS, 응답시간)
- [ ] 기준선 미달 시 배포 차단

**목표:**
- 모든 주요 API에 대해 최소 VU 100, 1분 테스트 필수
- p(95) 응답시간, TPS, 에러율 자동 검증

#### 🔄 3. 모니터링 알람 개선 (예정: 2026-01-04)
**담당**: DevOps팀

**추가할 알람:**
```yaml
알람 1: TPS 급감
  - 조건: 5분 평균 TPS가 기준선 대비 50% 이하
  - 심각도: Critical
  - 채널: Slack #alert + 전화

알람 2: 응답시간 급증
  - 조건: p(95) > 2초가 3분 이상 지속
  - 심각도: Warning
  - 채널: Slack #alert

알람 3: 에러율 증가
  - 조건: 5분 에러율 > 10%
  - 심각도: Critical
  - 채널: Slack #alert + 전화

알람 4: Connection Pool 고갈
  - 조건: Active Connection > 80%
  - 심각도: Warning
  - 채널: Slack #alert
```

#### 🔄 4. 성능 대시보드 구축 (예정: 2026-01-08)
**담당**: DevOps팀

**Grafana 대시보드 구성:**
- API별 TPS 실시간 차트
- API별 p(50), p(95), p(99) 응답시간
- 에러율 추이
- DB Connection Pool 사용률
- Redis Lock 대기 큐 크기

---

### Long-term (1개월 내)

#### 🎯 1. 배포 프로세스 개선 (예정: 2026-01-31)
**담당**: Tech Lead + DevOps

**새로운 배포 프로세스:**
```
1. 개발 완료
2. 단위 테스트 작성 (필수)
3. 통합 테스트 (필수)
4. 코드 리뷰 (필수)
5. ⭐ 부하 테스트 (필수) ← 새로 추가
   - 자동: CI/CD에서 실행
   - 수동: k6 스크립트 실행 결과 첨부
6. QA 검증 (필수)
7. 스테이징 배포 + 1시간 모니터링
8. 프로덕션 배포
9. ⭐ 배포 후 모니터링 (30분 필수) ← 새로 추가
```

**체크리스트:**
- [ ] 부하 테스트 결과 첨부 없으면 PR Merge 불가
- [ ] 성능 기준선 미달 시 배포 불가
- [ ] 배포 후 30분 모니터링 필수

#### 🎯 2. APM 도구 도입 (예정: 2026-02-15)
**담당**: DevOps팀

**도입 검토 도구:**
- Pinpoint (네이버, 무료 오픈소스)
- Datadog (상용, 월 $100~)
- New Relic (상용)

**기대 효과:**
- 실시간 트랜잭션 추적
- 병목 구간 자동 탐지
- 장애 발생 시 빠른 원인 파악 (35분 → 5분 목표)

#### 🎯 3. 아키텍처 개선 (예정: 2026-02-28)
**담당**: 백엔드팀

**계획:**

1. **Read Replica 도입**
   - 읽기 작업을 Replica로 분산
   - 장바구니 조회, 상품 조회 등 Read 부하 감소
   - 예상 효과: Write Master 부하 50% 감소

2. **CQRS 패턴 적용 (주문 도메인)**
   - Command (주문 생성): Write DB
   - Query (주문 조회): Read DB + Redis Cache
   - 읽기/쓰기 최적화 분리

3. **비동기 처리 확대**
   - 주문 완료 후 알림 발송 비동기화
   - 상품 랭킹 업데이트 비동기화
   - 트랜잭션 시간 단축 (현재 130ms → 50ms 목표)

#### 🎯 4. 장애 대응 매뉴얼 정립 (예정: 2026-01-20)
**담당**: DevOps + 백엔드팀

**작성할 문서:**
```
1. 장애 대응 프로세스
   - 장애 등급 분류 (P0~P3)
   - 등급별 대응 절차
   - 에스컬레이션 체계

2. 주요 장애 시나리오별 대응 매뉴얼
   - DB Connection Pool 고갈
   - Redis 장애
   - TPS 급감
   - 메모리 누수 / OOM

3. 온콜 담당자 지정
   - 주간 (09:00~18:00): 백엔드 3명 로테이션
   - 야간 (18:00~09:00): 백엔드 1명 + DevOps 1명
```

---

## 📊 개선 전후 비교

### 성능 지표 개선

| 지표 | 장애 시점 | 개선 후 | 개선율 |
|------|----------|---------|--------|
| **TPS** | 7.54 req/s | 81.68 req/s | **10.8배 ↑** |
| **평균 응답시간** | 8,200ms | 72ms | **99.1% ↓** |
| **p(95) 응답시간** | 30,000ms | 131ms | **99.6% ↓** |
| **p(99) 응답시간** | 60,000ms+ | 245ms | **99.6% ↓** |
| **성공률** | 14.1% | 91.4% | **6.5배 ↑** |
| **에러율** | 85.9% | 8.6% | **90% ↓** |
| **동시 처리 능력** | 1명 | 100명 | **100배 ↑** |

### 비즈니스 지표 개선 (추정)

| 지표 | Before | After | 개선 효과 |
|------|--------|-------|----------|
| 점심시간 주문 성공률 | 14% | 91% | 매출 6.5배 증가 |
| 평균 주문 완료 시간 | 30초 이상 | 0.2초 | 사용자 경험 개선 |
| 고객센터 문의 | 340건/시간 | 40건/시간 | 운영 비용 85% 감소 |
| 앱 리뷰 평점 | 2.1점 | 4.3점 (1주 후) | 브랜드 신뢰도 회복 |

### 기술 부채 해소

| 항목 | Before | After |
|------|--------|-------|
| Lock 전략 | 전역 락 (비효율) | 사용자별 락 (최적) |
| DB 인덱스 | cart_items 인덱스 없음 | idx_cart_id 추가 |
| Connection Pool | 10 (부족) | 100 (충분) |
| 부하 테스트 | 없음 | 3개 API 검증 완료 |
| 모니터링 | 기본 지표만 | TPS, p(95) 알람 추가 |

---

## 🤔 회고

### 잘한 점 (Keep)

1. **신속한 장애 인지 및 대응**
   - 장애 인지(14:10) → 배포(14:30) → 해소(14:35): **25분 대응**
   - 일반적인 장애 대응 시간(1~2시간) 대비 매우 빠름
   - 온콜 체계가 잘 작동함

2. **명확한 근본 원인 파악**
   - Redis Lock 대기 큐 직접 확인하여 증거 확보
   - 5-Whys 분석으로 근본 원인까지 추적
   - 단순 증상 치료가 아닌 근본 해결

3. **안전한 핫픽스 배포**
   - 긴급 상황에서도 테스트 및 리뷰 진행
   - 스테이징 검증 후 Blue-Green 배포
   - 단계적 트래픽 전환으로 리스크 최소화

4. **장애 발생 후 즉시 부하 테스트 실시**
   - 재발 방지를 위해 3개 주요 API 부하 테스트 완료
   - 다른 잠재적 문제도 사전 발견 및 해결
   - 이번 장애를 계기로 성능 문화 개선 시작

### 아쉬운 점 (Problem)

1. **배포 전 부하 테스트 미실시**
   - **가장 큰 실수**: 부하 테스트만 했어도 배포 전 발견 가능
   - 35분 장애, 2,500만원 손실, 고객 불만 → 모두 예방 가능했음
   - "기능 동작 = 배포 OK" 문화의 문제

2. **코드 리뷰에서 Lock 범위 미검토**
   - "동시성 제어가 있다" 확인만 하고 통과
   - "Lock의 범위가 적절한가?" 질문 누락
   - 성능 관점의 리뷰 부족

3. **모니터링 알람 지연**
   - 장애 발생(14:00) → 알람(14:07): **7분 지연**
   - 고객 문의로 먼저 인지 (14:05)
   - TPS 급감, 응답시간 급증에 대한 알람 없음

4. **성능 검증이 선택 사항**
   - 배포 프로세스에 부하 테스트 없음
   - QA 팀의 성능 테스트 체크리스트 미흡
   - "장애 나봐야 안다" 문화

### 배운 점 (Lesson Learned)

1. **Lock Granularity가 성능의 핵심**
   - 전역 락 vs 사용자별 락: 100배 성능 차이
   - "안전하게 넓게" < "필요한 만큼만"
   - 동시성 제어 설계 시 항상 Lock 범위 고민 필요

2. **부하 테스트는 선택이 아닌 필수**
   - 로컬 테스트로는 동시성 문제 절대 발견 불가
   - VU 50만 해도 대부분의 병목 발견 가능
   - "한 번의 부하 테스트 > 열 번의 코드 리뷰"

3. **모니터링은 장애 대응의 시작**
   - 알람이 늦으면 대응도 늦음
   - 7분 지연이 고객 신뢰도에 치명적 영향
   - TPS, p(95), 에러율 실시간 알람 필수

4. **장애는 시스템 개선의 기회**
   - 이번 장애를 계기로 부하 테스트 필수화
   - 모니터링 체계 개선
   - 성능 문화 정립의 시작점

### 다음에 시도할 것 (Try)

1. **모든 배포에 부하 테스트 필수화**
   - CI/CD 파이프라인에 k6 통합
   - 기준선 미달 시 PR Merge 차단
   - 개발 단계부터 성능 고려 문화

2. **성능 중심의 코드 리뷰**
   - 리뷰 체크리스트에 "Lock 범위 적절성" 추가
   - "동시성 제어 방법이 성능에 미치는 영향" 검토
   - Tech Lead의 성능 리뷰 필수

3. **실시간 모니터링 및 알람 강화**
   - TPS 급감 알람 (기준선 대비 50% 하락 시)
   - 응답시간 알람 임계값 조정 (p95 > 2초)
   - Slack + 전화 이중 알람

4. **정기적인 부하 테스트 훈련**
   - 월 1회 주요 API 부하 테스트
   - 성능 트렌드 분석 (개선/악화 추적)
   - 장애 대응 시뮬레이션 훈련

---

## 📚 참고 자료

### 관련 문서
- [부하 테스트 종합 보고서](./LOAD_TEST_TOTAL.md)
- [주문 API 성능 최적화 보고서](./PERFORMANCE_OPTIMIZATION_REPORT.md)
- [분산 락 설계 문서](../../document/DISTRIBUTED_LOCK_DESIGN.md)

### 코드 변경 이력
- PR #245: 긴급 핫픽스 - 전역 락 → 사용자별 락 변경
- Commit: 5b44478 - Connection Pool 확대 및 인덱스 추가

### 외부 참고 자료
- [우아한형제들 - 장애 대응 문화](https://techblog.woowahan.com/4886/)
- [LINE - 플랫폼 서버 장애 대응 프로세스](https://engineering.linecorp.com/ko/blog/line-platform-server-outage-process-and-dev-culture)
- [Redis Lock 성능 최적화 전략](https://redis.io/docs/manual/patterns/distributed-locks/)

---

## 🔖 부록

### A. 장애 시점 주요 메트릭

```
=== 장애 발생 시점 (14:00 ~ 14:35) ===

HTTP 메트릭:
- 총 요청: 4,856건
- 성공 (200 OK): 680건 (14.1%)
- 타임아웃 (504): 3,845건 (79.2%)
- 서버 오류 (500): 331건 (6.8%)

응답 시간 분포:
- p(50): 4,500ms
- p(90): 15,000ms
- p(95): 30,000ms
- p(99): 60,000ms+
- 최대: 60,000ms (클라이언트 타임아웃)

Redis Lock 메트릭:
- Lock 대기 큐 크기: 4,856
- 평균 Lock 획득 대기: 7,800ms
- 최대 Lock 획득 대기: 58,000ms
- Lock 보유 시간: 평균 130ms

DB 메트릭:
- Connection Pool 사용률: 100% (10/10)
- Connection 대기 큐: 90~95명 상시 대기
- 평균 Connection 대기 시간: 50ms
- Slow Query: 없음
```

### B. 고객 문의 분석

```
총 문의: 340건 (14:00 ~ 14:40)

문의 유형:
1. "주문 버튼 클릭했는데 계속 로딩": 178건 (52%)
2. "주문이 안 돼요": 95건 (28%)
3. "결제는 됐는데 주문 내역 없음": 18건 (5%)
4. "중복 결제 됐어요" (오인): 18건 (5%)
5. "환불 요청": 15건 (4%)
6. 기타: 16건 (5%)

고객 만족도 (CSAT):
- 장애 전 (12월 평균): 4.2/5.0
- 장애 당일: 2.1/5.0
- 장애 해소 후 1주: 3.8/5.0 (회복 중)
```

### C. 배포 타임라인

```
14:25:00 - 핫픽스 코드 작성 완료
14:25:30 - 로컬 단위 테스트 실행 (PASS)
14:26:00 - 로컬 k6 부하 테스트 실행 (VU 100, 30초)
14:26:30 - 부하 테스트 결과 확인 (TPS 82 달성)
14:27:00 - Tech Lead 긴급 코드 리뷰 시작
14:28:00 - 코드 리뷰 승인 (LGTM)
14:28:10 - 스테이징 환경 배포 시작
14:28:45 - 스테이징 배포 완료
14:29:00 - 스테이징 Smoke Test (성공)
14:29:30 - 프로덕션 배포 시작 (Blue-Green)
14:29:30 - Green 서버 시작
14:29:50 - Green 서버 헬스체크 통과
14:30:00 - 트래픽 전환 10% → Green
14:30:20 - 트래픽 전환 50% → Green
14:30:40 - 트래픽 전환 100% → Green
14:31:00 - Blue 서버 종료
14:32:00 - 성능 지표 확인 (TPS 82, p95 131ms)
14:35:00 - 장애 해소 선언
```

---

**문서 작성**: 2025-12-30 16:00
**최종 검토**: 백엔드 팀장, Tech Lead
**승인**: CTO
**다음 액션**: 배포 프로세스 개선 TF 구성 (2026-01-02)
