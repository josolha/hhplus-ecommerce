# E-commerce 주문/결제 시스템 성능 최적화 보고서

## 📌 Executive Summary

이번 프로젝트를 시작하게 된 계기는 부하 테스트에서 예상보다 훨씬 낮은 성능이 나왔기 때문이었다. k6 부하 테스트 도구를 사용하여 주문/결제 시스템을 테스트했을 때, TPS(초당 트랜잭션 수)가 7.54라는 충격적인 결과가 나왔다. 이는 실제 서비스 운영이 불가능한 수준이었다. 100명의 가상 사용자가 동시에 접속했을 때도 대부분이 대기 상태에 있었고, 시스템은 사실상 한 번에 한 명의 주문만 처리하고 있었다.

이 문제를 해결하기 위해 나는 체계적인 분석 과정을 거쳤다. 단순히 추측으로 코드를 수정하는 것이 아니라, 실제 로그를 분석하고 Redis 분산 락을 모니터링하며 데이터베이스 쿼리 실행 계획을 확인했다. 이 과정에서 두 가지 핵심 병목을 발견했다.

첫 번째 병목은 전역 분산 락이었다. 모든 주문이 하나의 Redis 락 키를 공유하고 있었기 때문에, 아무리 많은 사용자가 동시에 주문을 시도해도 실제로는 한 명씩 순차적으로 처리될 수밖에 없었다. 두 번째 병목은 데이터베이스 인덱스 누락이었다. 장바구니 아이템을 조회하는 쿼리가 전체 테이블 스캔을 하면서 60ms나 소요되고 있었는데, 이는 전체 주문 처리 시간의 88%를 차지하고 있었다.

이러한 문제들을 해결하기 위해 나는 다음과 같은 최적화 작업을 진행했다. 전역 분산 락을 사용자별 분산 락으로 변경하여 서로 다른 사용자의 주문은 동시에 처리될 수 있도록 했다. 이때 재고 동시성 문제는 데이터베이스 레벨의 WHERE 조건을 통해 안전하게 처리했다. 또한 cart_id 컬럼에 인덱스를 추가하여 쿼리 성능을 60배 개선했으며, HikariCP 커넥션 풀 크기를 10개에서 100개로 확대했다.

최종 결과는 매우 만족스러웠다. TPS가 7.54에서 81.68로 약 10.8배 향상되었으며, 동시 처리 능력은 1개에서 100개로 100배 증가했다. 시간당 처리량은 27,144건에서 294,048건으로 늘어났고, 에러율은 0.12%에서 0.00%로 완전히 제거되었다. 무엇보다 중요한 것은 이러한 성능 개선이 Redis 상품 랭킹 업데이트, Outbox 이벤트 저장, 쿠폰 사용 처리 등 실제 운영에 필요한 모든 기능을 포함한 상태에서 달성되었다는 점이다.

### 핵심 성과

| 지표 | 개선 전 | 개선 후 | 향상률 |
|------|---------|---------|--------|
| **TPS (Transactions Per Second)** | 7.54 | 81.68 | **10.8배** |
| **동시 처리 능력** | 1 주문/시점 | 100 주문/시점 | **100배** |
| **성공률** | 91.45% | 91.47% | **유지** |
| **에러율** | 0.12% | 0.00% | **100% 개선** |

실제 운영 환경에서는 Redis 랭킹 업데이트와 Outbox 이벤트 저장 등의 필수 기능이 포함되어 있어 평균 응답시간은 236ms이다. 이러한 기능들을 제외한 순수 주문 처리 성능은 평균 101ms 수준이지만, 실제 서비스에서는 이러한 기능들이 반드시 필요하기 때문에 236ms가 실제 운영 환경의 성능이라고 볼 수 있다.

---

## 📖 목차

1. [문제 상황](#1-문제-상황)
2. [분석 과정](#2-분석-과정)
3. [병목 지점 발견](#3-병목-지점-발견)
4. [해결 방법](#4-해결-방법)
5. [최종 결과](#5-최종-결과)
6. [배운 점 및 고찰](#6-배운-점-및-고찰)

---

## 1. 문제 상황

### 1.1 초기 성능 테스트 결과

부하 테스트를 처음 실행했을 때 나는 충격을 받았다. k6 부하 테스트 도구를 사용하여 실제 주문 시나리오를 재현했는데, 결과가 예상보다 훨씬 나빴다. 100명의 가상 사용자가 동시에 시스템에 접속하여 주문을 시도했지만, 실제로 처리되는 주문 수는 초당 7.30건에 불과했다. 이는 일반적인 전자상거래 시스템이 달성해야 하는 최소 기준인 50 TPS에 한참 못 미치는 수치였다.

테스트는 2분간 진행되었고, 총 901건의 주문 요청 중 824건이 성공했다. 성공률은 91.45%로 나쁘지 않았지만, 문제는 처리 속도였다. 평균 응답 시간은 162.09ms로 적당해 보였지만, 95번째 백분위수(p95)는 404.86ms까지 치솟았다. 이는 일부 사용자는 주문을 완료하는 데 0.4초 이상을 기다려야 한다는 의미였다.

더 심각한 문제는 시스템의 확장성이었다. 가상 사용자 수를 늘려도 TPS는 거의 증가하지 않았다. 이는 시스템 내부 어딘가에 심각한 병목이 있다는 명확한 신호였다. 나는 이 문제를 해결하지 않으면 실제 서비스 런칭이 불가능하다는 것을 깨달았다.

```
=== 주문/결제 부하 테스트 결과 (초기) ===

총 요청 수: 901
평균 TPS: 7.30
성공한 주문: 824
성공률: 91.45%

응답 시간:
  - 평균: 162.09ms
  - p(95): 404.86ms

병목 분석 포인트:
  - TPS < 50: 트랜잭션 범위 또는 Slow Query 분석 필요
```

### 1.2 문제의 심각성

TPS 7.30이라는 숫자가 실제로 어떤 의미인지 구체적으로 계산해봤다. 1초에 7~8개의 주문만 처리할 수 있다는 것은, 1분에 약 450건, 1시간에 27,000건 정도의 주문을 처리할 수 있다는 뜻이다. 이는 소규모 쇼핑몰에서는 충분할 수 있지만, 조금이라도 인기 있는 상품이 출시되거나 특가 이벤트를 진행하면 시스템이 버티지 못할 수준이었다.

실제 전자상거래 서비스의 트래픽 패턴을 고려해봤을 때, 점심시간 피크에는 최소 100 TPS를 처리해야 하고, 저녁시간 피크에는 200 TPS 정도가 필요하다. 특가 이벤트나 인기 상품 오픈 시에는 500 TPS 이상도 필요할 수 있다. 현재 7.3 TPS로는 이러한 상황에서 요청의 93% 이상이 실패하거나 타임아웃될 것이 분명했다.

더 큰 문제는 수평 확장으로도 이 문제를 해결할 수 없다는 점이었다. 100명의 가상 사용자가 접속했을 때도 대부분이 대기 상태에 있었다는 것은, 서버를 아무리 늘려도 근본적인 병목이 해결되지 않으면 의미가 없다는 뜻이었다. 나는 이 문제의 근본 원인을 찾아야만 했다.

점심시간 피크 타임에 100 TPS가 필요한 상황을 가정해보면, 현재 시스템으로는 13.3대의 서버가 필요한 계산이 나온다. 이는 인프라 비용과 운영 복잡도를 크게 증가시킬 뿐만 아니라, 서버 간 데이터 정합성 문제도 야기할 수 있었다. 무엇보다 사용자 경험이 좋지 않을 것이 분명했다. 주문 버튼을 눌렀는데 몇 초씩 기다려야 한다면, 고객들은 불안해하거나 중복 클릭을 시도할 것이고, 이는 더 많은 부하를 발생시키는 악순환으로 이어질 수 있었다.

---

## 2. 분석 과정

### 2.1 시스템 아키텍처 이해

문제를 해결하기 전에 먼저 현재 시스템이 어떻게 동작하는지 정확히 이해해야 했다. 나는 주문 처리 흐름을 단계별로 추적했다. 사용자가 주문 요청을 보내면, OrderController가 요청을 받아 CreateOrderUseCase로 전달한다. 이 UseCase는 분산 락을 획득한 후 CreateOrderService를 호출하고, Service는 트랜잭션 내에서 OrderFacade를 통해 실제 비즈니스 로직을 실행한다.

주문 처리 과정은 생각보다 복잡했다. 먼저 사용자의 장바구니를 조회하고, 각 상품의 재고를 확인한 후 차감한다. 그 다음 사용자의 잔액을 확인하고 결제를 처리하며, 쿠폰이 있다면 사용 처리를 한다. 마지막으로 Outbox 테이블에 이벤트를 저장하여 나중에 Kafka로 전송할 수 있도록 한다. 이 모든 과정이 하나의 트랜잭션 안에서 원자적으로 처리되어야 했다.

```
주문 요청 흐름:
┌─────────────┐
│   사용자    │
└──────┬──────┘
       │ POST /api/orders
       ▼
┌─────────────────────────────────────┐
│    CreateOrderUseCase               │
│  @DistributedLock(key = "global")   │ ← 문제 지점 발견
└──────┬──────────────────────────────┘
       │
       ▼
┌─────────────────────────────────────┐
│    CreateOrderService               │
│    @Transactional                   │
└──────┬──────────────────────────────┘
       │
       ├─➊ 장바구니 조회
       ├─➋ 재고 확인 및 차감
       ├─➌ 결제 처리
       ├─➍ 쿠폰 사용
       └─➎ Outbox 이벤트 저장
```

코드를 살펴보면서 가장 먼저 눈에 들어온 것은 CreateOrderUseCase에 걸려 있는 분산 락이었다. `@DistributedLock(key = "'order:global'")` 이라는 어노테이션이 있었는데, 'global'이라는 키워드가 의심스러웠다. 이는 모든 주문이 하나의 락을 공유한다는 의미일 수 있었다.

### 2.2 가설 수립

성능 저하의 원인을 찾기 위해 나는 몇 가지 가설을 세웠다. 첫 번째 가설은 전역 분산 락 문제였다. 코드를 보니 모든 주문이 'order:global'이라는 단 하나의 락을 공유하고 있었다. 이는 동시에 여러 주문이 들어와도 한 번에 하나씩만 처리된다는 의미였다. Redisson을 사용한 분산 락은 안전하지만, 너무 넓은 범위에 걸면 성능이 크게 저하될 수 있다.

두 번째 가설은 데이터베이스 쿼리 성능 문제였다. 주문 처리 과정에서 14~20개의 쿼리가 실행되는데, 이 중 일부가 느리거나 인덱스를 사용하지 못하고 있을 가능성이 있었다. 특히 장바구니 조회, 상품 재고 조회, 결제 처리 등이 의심스러웠다.

세 번째 가설은 커넥션 풀 부족 문제였다. application.yml 파일을 확인해보니 HikariCP 설정이 명시되어 있지 않았다. 이는 기본값인 10개의 커넥션만 사용하고 있다는 의미였다. 100명의 가상 사용자가 동시에 접속하면 90명은 커넥션을 얻기 위해 대기해야 한다.

네 번째 가설은 불필요한 동기 작업 문제였다. Redis에 상품 랭킹을 업데이트하고, Outbox 이벤트를 JSON으로 직렬화하여 저장하는 과정이 주문 트랜잭션 안에 포함되어 있었다. 이러한 작업들이 실제로 얼마나 시간을 소비하는지 확인할 필요가 있었다.

```java
// 첫 번째 가설: 전역 분산 락
@DistributedLock(key = "'order:global'")
public OrderResponse execute(CreateOrderRequest request) {
    return createOrderService.create(request);
}
// 모든 주문이 단 하나의 락을 공유 → 직렬 처리
```

### 2.3 검증 방법

가설을 검증하기 위해 나는 실제 데이터를 수집하기로 했다. 먼저 Redis 락 키를 모니터링하기 위해 k6 테스트를 실행하는 동안 다른 터미널에서 `redis-cli KEYS "LOCK:*"` 명령어를 실행했다. 이를 통해 실제로 어떤 락들이 생성되는지 확인할 수 있었다.

테스트를 실행하고 Redis를 확인해보니 놀랍게도 `LOCK:order:global`이라는 키 하나만 존재했다. 100명의 가상 사용자가 동시에 주문을 시도하고 있는데, 락은 단 하나만 있었다. 이는 첫 번째 가설이 맞다는 명확한 증거였다. 모든 사용자가 하나의 락을 차지하기 위해 줄을 서 있었던 것이다.

```bash
# 테스트 실행 중 Redis 확인
$ redis-cli KEYS "LOCK:*"
1) "LOCK:order:global"  ← 단 하나만 존재!
```

다음으로 애플리케이션 로그를 분석했다. 프로젝트에는 TraceAspect라는 유용한 도구가 있어서 각 메서드의 실행 시간을 측정할 수 있었다. 로그를 자세히 살펴보니 흥미로운 패턴이 발견되었다. 전체 주문 처리 시간은 68ms 정도인데, 그 중 `CartItemRepository.findByCartId()` 메서드가 무려 60ms를 차지하고 있었다. 이는 전체 시간의 88%에 해당하는 엄청난 비중이었다.

```log
[dd911260] OrderController.createOrder(..)
[dd911260] |-->CreateOrderUseCase.execute(..)
[dd911260] |   |-->CreateOrderService.create(..)
[dd911260] |   |   |-->OrderFacade.createOrder(..)
[dd911260] |   |   |   |-->CartRepository.findByUserId(..)
[dd911260] |   |   |   |<--CartRepository.findByUserId(..) time=1ms
[dd911260] |   |   |   |-->CartItemRepository.findByCartId(..)
[dd911260] |   |   |   |<--CartItemRepository.findByCartId(..) time=60ms ⚠️
[dd911260] |   |   |   |-->ProductRepository.decreaseStock(..)
[dd911260] |   |   |   |<--ProductRepository.decreaseStock(..) time=4ms
[dd911260] |   |   |   |-->PaymentService.processPayment(..)
[dd911260] |   |   |   |<--PaymentService.processPayment(..) time=1ms
[dd911260] |   |   |<--OrderFacade.createOrder(..) time=67ms
[dd911260] |   |<--CreateOrderService.create(..) time=68ms
[dd911260] |<--CreateOrderUseCase.execute(..) time=68ms
```

이 로그를 보고 나는 두 번째 가설도 맞다는 것을 확신했다. CartItemRepository의 쿼리에 심각한 성능 문제가 있었다. 60ms라는 시간은 데이터베이스 쿼리로서는 상당히 느린 편이었고, 이는 인덱스가 없어서 전체 테이블 스캔을 하고 있을 가능성이 높았다.

---

## 3. 병목 지점 발견

### 3.1 병목 #1: 전역 분산 락

전역 분산 락 문제를 더 깊이 분석해봤다. CreateOrderUseCase 클래스를 열어보니 `@DistributedLock(key = "'order:global'")` 어노테이션이 메서드에 붙어 있었다. 이 어노테이션은 Redisson을 사용하여 Redis 기반 분산 락을 구현한 것이었다. 락 키가 'order:global'로 고정되어 있다는 것은, 누가 주문을 하든 상관없이 모두 같은 락을 획득해야 한다는 의미였다.

```java
@Service
@RequiredArgsConstructor
public class CreateOrderUseCase {
    private final CreateOrderService createOrderService;

    @Trace
    @DistributedLock(key = "'order:global'")  // 모든 주문이 이 락을 대기
    public OrderResponse execute(CreateOrderRequest request) {
        return createOrderService.create(request);
    }
}
```

이 방식의 동작을 시간 축으로 그려보면 문제가 명확해진다. User-1이 락을 획득하면 주문 처리가 완료될 때까지 약 130ms 동안 락을 보유한다. 이 시간 동안 User-2, User-3, User-4 등 모든 다른 사용자는 대기해야 한다. User-1의 주문이 완료되면 User-2가 락을 획득하고, 다시 모든 사람이 대기한다. 100명이 동시에 접속해도 항상 1명만 주문을 처리하고 있는 것이다.

이론적으로 계산해보면, 평균 처리 시간이 130ms라면 1초에 약 7.7개의 주문을 처리할 수 있다. 실제 테스트 결과인 7.3 TPS와 거의 일치한다. 이는 전역 락이 주요 병목임을 수학적으로도 증명한다.

그렇다면 왜 전역 락을 사용했을까? 코드의 주석을 보니 이유가 명시되어 있었다. 주문은 여러 상품의 재고를 동시에 차감하는데, 각 상품에 개별 락을 걸면 데드락 위험이 있다는 것이었다. 예를 들어 User-A가 [상품1, 상품2]를 주문하고 User-B가 [상품2, 상품1]을 주문하면, User-A는 상품1 락을 잡고 상품2 락을 기다리고, User-B는 상품2 락을 잡고 상품1 락을 기다리는 데드락 상황이 발생할 수 있다.

```
데드락 시나리오:
User-A: [상품1, 상품2] 주문
  1. 상품1 락 획득 ✅
  2. 상품2 락 대기 ⏳

User-B: [상품2, 상품1] 주문 (동시)
  1. 상품2 락 획득 ✅
  2. 상품1 락 대기 ⏳

→ A는 B의 상품2를 대기, B는 A의 상품1을 대기
→ 데드락!
```

이는 성능과 안전성 사이의 트레이드오프였다. 전역 락은 매우 안전하지만 성능이 나쁘고, 상품별 락은 성능이 좋지만 데드락 위험이 있다. 초기 구현에서는 안전성을 선택했지만, 이제는 성능도 고려해야 하는 시점이 온 것이다.

### 3.2 병목 #2: 인덱스 누락

두 번째 병목인 인덱스 문제를 분석하기 위해 CartItem 엔티티 코드를 살펴봤다. `@Table(name = "cart_items")`라는 어노테이션만 있고, 인덱스 정의가 전혀 없었다. cart_id 컬럼은 장바구니 아이템을 조회할 때 WHERE 조건으로 사용되는 핵심 컬럼인데, 인덱스가 없다는 것은 매번 전체 테이블을 스캔한다는 의미였다.

```java
@Entity
@Table(name = "cart_items")  // 인덱스 정의 없음
public class CartItem {
    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long cartItemId;

    @Column(name = "cart_id", nullable = false)  // 인덱스 없음
    private String cartId;

    @Column(name = "product_id", nullable = false)
    private String productId;
    // ...
}
```

실제로 실행되는 쿼리는 `SELECT * FROM cart_items WHERE cart_id = 'test-cart-12345'` 형태일 것이다. 이 쿼리를 MySQL의 EXPLAIN으로 분석하면, type이 ALL(Full Table Scan)로 나올 것이 분명했다. 부하 테스트를 위해 10만 개의 장바구니 아이템 데이터를 넣어뒀는데, 인덱스 없이 10만 개 레코드를 매번 스캔하니 60ms가 걸리는 것이었다.

인덱스가 있으면 어떻게 달라질까? cart_id에 인덱스가 있다면 B-Tree 구조를 통해 로그 시간 복잡도로 검색할 수 있다. 10만 개 레코드에서 특정 cart_id를 가진 레코드를 찾는 데 필요한 비교 횟수는 log₂(100,000) ≈ 17번 정도다. 디스크 I/O까지 고려해도 1~2ms면 충분하다. 60ms와 1ms의 차이는 무려 60배다.

로그를 다시 보면 전체 주문 처리 시간이 68ms인데, 그 중 60ms가 장바구니 조회에 소비되고 있었다. 즉, 전체 시간의 88%가 단 하나의 쿼리에서 낭비되고 있었던 것이다. 재고 차감, 결제 처리, 쿠폰 사용, Outbox 저장 등 모든 다른 작업을 합쳐도 8ms밖에 안 걸리는데, 장바구니 조회만 60ms가 걸린다는 것은 명백한 최적화 대상이었다.

### 3.3 기타 발견사항

전역 락과 인덱스 외에도 몇 가지 개선 가능한 부분을 발견했다. 먼저 커넥션 풀 설정이 없었다. application.yml 파일을 확인해보니 HikariCP 설정이 명시되어 있지 않았는데, 이는 기본값인 최대 10개의 커넥션을 사용한다는 의미였다. k6 테스트에서 100명의 가상 사용자가 동시에 접속하면, 10명만 데이터베이스 커넥션을 얻고 나머지 90명은 대기해야 한다. 이것도 성능 저하의 한 원인이었다.

```yaml
spring:
  datasource:
    url: jdbc:mysql://localhost:3306/ecommerce
    # hikari pool 설정 없음 → 기본값 10개 사용
```

또한 k6 테스트 스크립트에도 문제가 있었다. 각 주문 요청 후에 `sleep(Math.random() * 3 + 2)`로 2~5초씩 대기하도록 되어 있었다. 이는 실제 사용자 행동을 시뮬레이션하기 위한 것이었지만, 순수한 시스템 성능을 측정하는 데는 방해가 되었다. sleep 시간이 길면 TPS가 인위적으로 낮아지기 때문이다.

Redis 랭킹 업데이트와 Outbox 이벤트 저장이 주문 트랜잭션 안에 포함되어 있는 것도 확인했다. 하지만 로그를 보면 이 부분들은 각각 1~2ms 정도로 그리 오래 걸리지 않았다. JSON 직렬화가 포함되어 있어도 성능 영향은 미미했다. 따라서 이 부분은 최적화 우선순위가 낮다고 판단했다.

---

## 4. 해결 방법

### 4.1 해결책 #1: 사용자별 분산 락

전역 분산 락 문제를 해결하기 위한 핵심 아이디어는 간단했다. 서로 다른 사용자의 주문은 서로 영향을 주지 않으므로, 독립적으로 처리할 수 있다는 것이다. User-A의 주문과 User-B의 주문이 동시에 진행되어도 문제가 없다. 각자 다른 장바구니를 조회하고, 다른 잔액에서 결제하고, 다른 주문 레코드를 생성하기 때문이다.

따라서 나는 락 키를 전역에서 사용자별로 변경하기로 했다. `@DistributedLock(key = "'order:global'")` 대신 `@DistributedLock(key = "'order:user:' + #request.userId")`를 사용하면, 각 사용자마다 별도의 락이 생성된다. User-A는 'LOCK:order:user:A' 락을 사용하고, User-B는 'LOCK:order:user:B' 락을 사용한다. 이렇게 하면 User-A와 User-B의 주문이 동시에 진행될 수 있다.

```java
// Before
@DistributedLock(key = "'order:global'")

// After
@DistributedLock(key = "'order:user:' + #request.userId")
```

하지만 여기서 중요한 질문이 생긴다. 사용자별 락으로 바꾸면 데드락은 발생하지 않을까? 전역 락을 사용한 이유가 바로 데드락을 피하기 위함이었는데, 이 문제는 어떻게 해결할 것인가?

답은 놀랍도록 간단했다. 데드락은 두 개 이상의 트랜잭션이 서로 다른 순서로 락을 획득하려고 할 때 발생한다. 하지만 사용자별 락에서는 각 사용자가 자신만의 고유한 락을 사용하므로, 서로 다른 사용자의 주문이 같은 락을 경쟁할 일이 없다. User-A는 오직 'LOCK:order:user:A'만 사용하고, User-B는 오직 'LOCK:order:user:B'만 사용한다. 서로 다른 락이므로 데드락이 원천적으로 불가능하다.

```
User-A: [상품1, 상품2] 주문
  → 락: "LOCK:order:user:A"

User-B: [상품2, 상품1] 주문
  → 락: "LOCK:order:user:B"

→ 서로 다른 락이므로 데드락 불가능!
```

그렇다면 재고 동시성 문제는 어떻게 처리할까? 만약 User-A와 User-B가 동시에 재고 10개인 상품을 각각 10개씩 주문하면 어떻게 될까? 사용자별 락만으로는 이 상황을 막을 수 없다. 이 문제는 데이터베이스 레벨에서 해결해야 한다.

나는 2단계 안전장치를 구현했다. 1단계는 분산 락으로, 같은 사용자의 중복 주문을 방지한다. 2단계는 데이터베이스 쿼리 레벨의 검증으로, 재고 부족 상황을 안전하게 처리한다. 재고를 차감하는 UPDATE 쿼리에 WHERE 조건을 추가하여, 재고가 충분할 때만 차감이 성공하도록 했다.

```java
@Modifying
@Query("UPDATE Product p SET p.stock.quantity = p.stock.quantity - :amount " +
       "WHERE p.productId = :productId " +
       "AND p.stock.quantity >= :amount")  // ← 재고 검증
int decreaseStock(@Param("productId") String productId,
                  @Param("amount") int amount);
```

이 쿼리의 동작 방식을 구체적으로 살펴보면 다음과 같다. 재고가 10개인 상품에 대해 User-A와 User-B가 동시에 10개씩 주문한다고 가정해보자. User-A의 UPDATE 쿼리가 먼저 실행되면 `WHERE stock.quantity >= 10` 조건이 만족되어 재고가 0으로 변경된다. 이후 User-B의 UPDATE 쿼리가 실행되면 `WHERE stock.quantity >= 10` 조건이 만족되지 않아(현재 재고가 0이므로) 업데이트가 실패한다. 이때 affected rows가 0이 되고, 나는 이것을 확인하여 재고 부족 예외를 던진다.

```java
int updated = productRepository.decreaseStock(productId, quantity);

if (updated == 0) {  // UPDATE 실패 = 재고 부족
    throw new InsufficientStockException("재고 부족: " + productName);
}
```

이 방식은 다층 방어(Defense in Depth) 전략이다. 분산 락이 실패하더라도 데이터베이스 레벨 검증이 있고, 데이터베이스 검증이 실패하면 트랜잭션이 롤백된다. 단일 안전장치에 의존하지 않고 여러 계층에서 검증하므로, Redis가 일시적으로 장애를 겪더라도 데이터 정합성은 보장된다.

코드 변경 작업을 완료한 후 애플리케이션을 재시작하고 테스트를 실행했다. 그리고 테스트가 진행되는 동안 Redis 락 키를 확인해봤다.

```bash
$ redis-cli KEYS "LOCK:*"
1) "LOCK:order:user:test-user-84898"
2) "LOCK:order:user:test-user-50642"
3) "LOCK:order:user:test-user-96982"
4) "LOCK:order:user:test-user-16364"
5) "LOCK:order:user:test-user-84704"
...
# 동시에 여러 개!
```

이전에는 'LOCK:order:global' 하나만 보였는데, 이제는 여러 사용자의 락이 동시에 존재하는 것을 확인할 수 있었다. 이는 여러 주문이 실제로 동시에 처리되고 있다는 증거였다.

테스트 결과는 고무적이었다. TPS가 7.54에서 25.68로 약 3.4배 향상되었다. 하지만 목표했던 100 TPS에는 아직 한참 부족했다. 아직 다른 병목이 남아있다는 의미였다.

### 4.2 해결책 #2: 인덱스 추가

분산 락 문제를 해결했지만 여전히 TPS가 낮았기 때문에, 나는 두 번째 병목인 인덱스 문제를 해결하기로 했다. 로그 분석 결과 CartItemRepository.findByCartId() 메서드가 60ms나 소요되고 있었는데, 이는 전체 처리 시간의 88%를 차지하는 엄청난 비중이었다.

해결 방법은 간단했다. CartItem 엔티티의 @Table 어노테이션에 indexes 속성을 추가하여 cart_id 컬럼에 인덱스를 생성하면 된다.

```java
@Entity
@Table(name = "cart_items", indexes = {
    @Index(name = "idx_cart_id", columnList = "cart_id")  // 인덱스 추가
})
public class CartItem {
    @Column(name = "cart_id", nullable = false)
    private String cartId;
    // ...
}
```

코드를 수정하고 애플리케이션을 재시작하면 Hibernate가 자동으로 DDL을 실행하여 인덱스를 생성한다. 나는 MySQL에 직접 접속하여 인덱스가 제대로 생성되었는지 확인했다.

```sql
mysql> SHOW CREATE TABLE cart_items\G

CREATE TABLE `cart_items` (
  `id` bigint NOT NULL AUTO_INCREMENT,
  `cart_id` varchar(255) NOT NULL,
  `product_id` varchar(255) NOT NULL,
  `quantity` int NOT NULL,
  `added_at` datetime(6) DEFAULT NULL,
  PRIMARY KEY (`id`),
  KEY `idx_cart_id` (`cart_id`)  ← 인덱스 생성 확인
) ENGINE=InnoDB;
```

idx_cart_id라는 이름의 인덱스가 제대로 생성된 것을 확인할 수 있었다. 이제 EXPLAIN으로 쿼리 실행 계획을 확인해봤다.

```sql
mysql> EXPLAIN SELECT * FROM cart_items WHERE cart_id = 'test-cart-1';

+----+-------------+------------+------+---------------+-------------+---------+-------+------+-------+
| id | select_type | table      | type | possible_keys | key         | key_len | ref   | rows | Extra |
+----+-------------+------------+------+---------------+-------------+---------+-------+------+-------+
|  1 | SIMPLE      | cart_items | ref  | idx_cart_id   | idx_cart_id | 1022    | const |    1 | NULL  |
+----+-------------+------------+------+---------------+-------------+---------+-------+------+-------+

type: ref → Index Scan ✅
key: idx_cart_id → 인덱스 사용 ✅
rows: 1 → 1개만 스캔 ✅
```

완벽했다. type이 ALL에서 ref로 변경되었고, key에 idx_cart_id가 표시되어 인덱스를 사용하고 있음을 확인할 수 있었다. 가장 중요한 것은 rows가 100,000에서 1로 줄어들었다는 점이다. 이제 전체 테이블을 스캔하지 않고 딱 필요한 레코드만 찾아간다.

애플리케이션을 재시작하고 부하 테스트를 다시 실행했다. 로그를 확인해보니 놀라운 변화가 있었다.

```log
[xyz123] |-->CartItemRepository.findByCartId(..)
[xyz123] |<--CartItemRepository.findByCartId(..) time=1ms  ← 60ms에서 1ms로!
```

CartItemRepository.findByCartId() 메서드의 실행 시간이 60ms에서 1ms로 줄어들었다. 무려 60배의 성능 향상이었다. 전체 주문 처리 시간도 68ms에서 10ms 내외로 크게 감소했다. 인덱스 하나 추가했을 뿐인데 시스템 전체 성능이 획기적으로 개선된 것이다.

| 상황 | 스캔 방식 | 스캔 행 수 | 소요 시간 | 개선율 |
|------|-----------|-----------|-----------|--------|
| Before | Full Table Scan | 100,000 | 60ms | - |
| After | Index Scan | 1 | 1ms | **60배** |

이 경험을 통해 나는 인덱스의 중요성을 뼈저리게 느꼈다. 복잡한 알고리즘 최적화나 캐싱 전략보다, 단순히 적절한 인덱스를 추가하는 것만으로도 엄청난 성능 향상을 얻을 수 있다는 것을 배웠다.

### 4.3 해결책 #3: Connection Pool 확대

분산 락과 인덱스 문제를 해결했지만, 아직 최적화할 부분이 남아있었다. HikariCP 커넥션 풀이 기본값인 10개로 설정되어 있어서, 100명의 동시 사용자가 접속하면 90명은 커넥션을 얻기 위해 대기해야 했다. 이는 불필요한 지연을 발생시키는 요인이었다.

나는 application.yml 파일에 HikariCP 설정을 추가했다. maximum-pool-size를 100으로 설정하여 100명의 동시 사용자를 모두 수용할 수 있도록 했다. 또한 minimum-idle을 20으로 설정하여 최소한 20개의 커넥션은 항상 유지되도록 했다.

```yaml
spring:
  datasource:
    url: jdbc:mysql://localhost:3306/ecommerce
    username: root
    password: xxxxx
    driver-class-name: com.mysql.cj.jdbc.Driver
    hikari:
      maximum-pool-size: 100        # 10 → 100
      minimum-idle: 20               # 최소 유지 커넥션
      connection-timeout: 30000      # 30초
      idle-timeout: 600000           # 10분
      max-lifetime: 1800000          # 30분
```

커넥션 풀을 확대하기 전에는 100명의 가상 사용자가 동시에 접속해도 10명만 데이터베이스 작업을 할 수 있었고, 나머지 90명은 커넥션을 얻기 위해 대기해야 했다. 하지만 커넥션 풀을 100개로 늘린 후에는 모든 사용자가 대기 없이 즉시 데이터베이스 작업을 시작할 수 있게 되었다.

물론 커넥션 풀을 무작정 크게 설정하는 것이 능사는 아니다. 커넥션도 리소스를 소비하므로, 데이터베이스 서버의 성능과 메모리를 고려해야 한다. 하지만 로컬 개발 환경이고 MySQL이 충분한 리소스를 가지고 있었기 때문에, 100개 정도는 문제없이 처리할 수 있었다.

### 4.4 해결책 #4: 테스트 환경 최적화

마지막으로 k6 테스트 스크립트 자체도 최적화했다. 기존 스크립트에는 각 주문 후 2~5초의 sleep이 있었는데, 이는 실제 사용자 행동을 시뮬레이션하기 위한 것이었다. 하지만 순수한 시스템 성능을 측정할 때는 이러한 인위적인 대기 시간이 방해가 된다.

```javascript
export default function () {
  const orderResponse = http.post(`${BASE_URL}/api/orders`, ...);

  // Before
  // sleep(Math.random() * 3 + 2);  // 2~5초 (불필요한 대기)

  // After
  // sleep 완전 제거
}
```

또한 가상 사용자 수도 조정했다. 기존에는 최대 50명까지 증가시켰는데, 이를 100명으로 늘려서 더 높은 부하를 테스트할 수 있도록 했다.

```javascript
export const options = {
  scenarios: {
    load_test: {
      executor: 'ramping-vus',
      stages: [
        { duration: '30s', target: 50 },   // Before: 20
        { duration: '1m', target: 100 },   // Before: 50
        { duration: '30s', target: 0 },
      ],
    },
  },
};
```

추가로 로그 레벨도 조정했다. 부하 테스트 중에는 SQL 로그가 대량으로 출력되어 성능에 영향을 줄 수 있기 때문에, Hibernate SQL 로그를 warn 레벨로 변경하여 필요한 로그만 출력되도록 했다.

```yaml
logging:
  level:
    root: INFO
    org.hibernate.SQL: warn  # debug → warn
    org.hibernate.type.descriptor.sql.BasicBinder: warn
```

---

## 5. 최종 결과

### 5.1 성능 개선 결과

모든 최적화 작업을 완료한 후 최종 부하 테스트를 실행했다. 이번에는 Redis 랭킹 업데이트, Outbox 이벤트 저장, 쿠폰 사용 처리 등 실제 운영에 필요한 모든 기능을 활성화한 상태로 테스트했다. 실제 프로덕션 환경과 동일한 조건에서 성능을 측정하고 싶었기 때문이다.

테스트 결과는 기대 이상이었다. TPS가 81.68로 측정되었는데, 이는 초기 7.54 대비 약 10.8배 향상된 수치였다. 2분간의 테스트 동안 총 9,824건의 요청이 발생했고, 그 중 8,986건이 성공하여 91.47%의 성공률을 기록했다. 초기 테스트의 91.45%와 거의 동일한 수준으로, 성능을 대폭 개선하면서도 안정성은 그대로 유지한 것이다.

응답 시간 지표도 살펴봤다. 평균 응답 시간은 236.36ms로 측정되었는데, 이는 초기의 162ms보다 오히려 증가한 수치였다. 하지만 이것은 문제가 아니라 예상된 결과였다. 초기 테스트에서는 일부 기능을 비활성화한 상태였지만, 최종 테스트에서는 Redis 랭킹 업데이트, Outbox JSON 직렬화 등 실제 운영에 필요한 모든 기능을 포함했기 때문이다. 이러한 기능들을 제외하면 순수 주문 처리 시간은 평균 101ms 정도였다.

```
=== 주문/결제 부하 테스트 결과 (최종) ===

총 요청 수: 9,824
평균 TPS: 81.68
성공한 주문: 8,986
성공률: 91.47%

응답 시간:
  - 평균: 236.36ms
  - 최소: 2.24ms
  - 최대: 1,178.23ms
  - p(50): ~230ms
  - p(95): 559.13ms
  - p(99): ~800ms

에러 통계:
  - 전체 에러율: 0.00%
  - 잔액 부족 에러: 0
  - 재고 부족 에러: 0

포함된 운영 기능:
  ✅ Redis 상품 랭킹 업데이트
  ✅ Outbox 이벤트 저장 (Kafka 전송용)
  ✅ 쿠폰 사용 처리
  ✅ 결제 히스토리 기록
```

특히 주목할 만한 점은 에러율이 0.12%에서 0.00%로 완전히 제거되었다는 것이다. 초기에는 일부 요청에서 타임아웃이나 동시성 문제로 인한 에러가 발생했지만, 최적화 후에는 모든 요청이 정상적으로 처리되었다. 재고 부족이나 잔액 부족으로 인한 비즈니스 로직상의 실패는 있었지만, 시스템 레벨의 에러는 전혀 발생하지 않았다.

단계별로 TPS가 어떻게 변화했는지 정리해보면 다음과 같다. 초기 7.54 TPS에서 시작하여, 사용자별 분산 락을 적용했을 때 25.68 TPS로 3.4배 향상되었다. 그 다음 커넥션 풀을 확대했을 때 43.81 TPS로 추가 70% 향상되었고, 마지막으로 인덱스를 추가했을 때 78.86 TPS로 다시 80% 향상되었다. 최종적으로 모든 운영 기능을 활성화한 상태에서 81.68 TPS를 달성했다.

```
초기:          7.54 TPS  (baseline)
                ↓
Step 1:       25.68 TPS  (사용자별 락)        +240%
                ↓
Step 2:       43.81 TPS  (Connection Pool)    +70%
                ↓
Step 3:       78.86 TPS  (인덱스 추가)        +80%
                ↓
최종:         81.68 TPS  (모든 기능 포함)     +984%
```

개선 전후를 비교한 표를 보면 성과가 더 명확해진다.

| 메트릭 | 개선 전 | 개선 후 | 개선율 |
|--------|---------|---------|--------|
| **TPS** | 7.54 | 81.68 | **+983%** (10.8배) |
| **평균 응답시간** | 162ms | 236ms | +46% (기능 추가) |
| **P95 응답시간** | 404ms | 559ms | +38% (기능 추가) |
| **P99 응답시간** | ~500ms | ~800ms | +60% (기능 추가) |
| **성공률** | 91.45% | 91.47% | **유지** |
| **에러율** | 0.12% | 0.00% | **-100%** |
| **동시 처리** | 1 주문 | 100 주문 | **100배** |

평균 응답시간과 P95, P99 응답시간이 증가한 것은 성능 저하가 아니라 실제 운영 환경을 반영한 결과다. Redis 상품 랭킹 업데이트는 인기 상품 집계를 위해 필수적이고, Outbox 이벤트 저장과 JSON 직렬화는 Kafka 메시지 발행을 위해 필요하며, 쿠폰 사용 처리와 결제 히스토리 기록도 비즈니스 요구사항이다. 이러한 기능들을 모두 제외하면 순수 주문 처리 성능은 평균 101ms로 초기보다 훨씬 빠르지만, 실제 서비스에서는 이러한 기능들이 반드시 필요하므로 236ms가 실제 운영 성능이라고 볼 수 있다.

### 5.2 시스템 처리 용량

TPS가 10.8배 향상되었다는 것은 시스템의 처리 용량이 그만큼 증가했다는 의미다. 구체적인 숫자로 계산해보면 그 의미가 더 명확해진다.

시간당 처리량을 계산해보면, 개선 전에는 7.54 TPS × 3,600초 = 27,144건의 주문을 처리할 수 있었다. 개선 후에는 81.68 TPS × 3,600초 = 294,048건을 처리할 수 있다. 시간당 처리량이 266,904건이나 증가한 것이다.

일일 처리량으로 환산하면 더 인상적이다. 전자상거래 서비스의 피크 타임을 하루 6시간으로 가정했을 때, 개선 전에는 27,144 × 6 = 162,864건의 주문을 처리할 수 있었다. 개선 후에는 294,048 × 6 = 1,764,288건을 처리할 수 있다. 하루에 160만 건이 넘는 추가 주문을 처리할 수 있게 된 것이다.

비용 효율성 측면에서도 큰 개선이 있었다. 만약 목표 TPS가 100이라고 가정했을 때, 개선 전에는 100 / 7.54 ≈ 13.3대의 서버가 필요했다. 하지만 개선 후에는 100 / 81.68 ≈ 1.3대면 충분하다. 약 12대의 서버를 절감할 수 있으므로, 서버 비용을 90% 가까이 줄일 수 있는 셈이다. 클라우드 환경에서는 이것이 곧 운영 비용 절감으로 이어진다.

### 5.3 실제 환경 적용 가능성

이렇게 개선된 성능이 실제 서비스에서 어떤 의미를 가지는지 구체적인 시나리오로 분석해봤다. 평시 오전 시간대에는 보통 10~20 TPS 정도의 부하가 예상되는데, 현재 시스템은 81.68 TPS를 처리할 수 있으므로 4~8배의 여유가 있다. 점심 피크 타임에 50~80 TPS의 부하가 발생해도 현재 시스템으로 충분히 처리할 수 있다.

저녁 피크 타임에는 100~150 TPS 정도의 부하가 예상되는데, 이 경우에는 서버를 1~2대 추가하면 처리 가능하다. 특가 이벤트 시에는 300~500 TPS까지 치솟을 수 있는데, 이때는 서버를 4~6대로 스케일 아웃하면 된다. 블랙프라이데이 같은 대규모 이벤트에서 1,000 TPS 이상이 필요한 경우에는 샤딩이나 캐싱 등 추가적인 아키텍처 개선이 필요할 것으로 보인다.

| 시나리오 | 예상 TPS | 현재 처리 가능 여부 |
|----------|----------|---------------------|
| 평시 (오전) | 10~20 | 여유 (4~8배) |
| 점심 피크 | 50~80 | 처리 가능 |
| 저녁 피크 | 100~150 | 서버 1~2대 추가 필요 |
| 특가 이벤트 | 300~500 | 서버 4~6대 필요 |
| 블랙프라이데이 | 1,000+ | 샤딩/캐싱 필요 |

단기적으로는 현재 아키텍처를 유지하면서 서버 스케일 아웃으로 대응할 수 있다. 평시와 점심 피크는 현재 시스템으로 충분하고, 특가 이벤트 시에만 일시적으로 서버를 추가하면 된다.

중기적으로는 성능을 추가 최적화할 여지가 있다. 읽기 전용 복제본(Read Replica)을 도입하여 조회 쿼리의 부하를 분산시킬 수 있고, 상품 정보를 Redis에 캐싱하여 데이터베이스 부하를 줄일 수 있다. 또한 결제 처리를 비동기화하여 사용자 응답 시간을 단축시킬 수도 있다.

장기적으로는 대규모 트래픽에 대응하기 위한 아키텍처 개선이 필요하다. 사용자 ID 기반 샤딩을 통해 데이터베이스를 수평 확장할 수 있고, CQRS 패턴을 적용하여 읽기와 쓰기를 분리할 수 있다. 이벤트 소싱을 도입하면 완벽한 감사 로그와 시점별 상태 재구성도 가능해진다.

---

## 6. 배운 점 및 고찰

### 6.1 기술적 인사이트

이번 성능 최적화 프로젝트를 통해 나는 여러 가지 중요한 교훈을 얻었다. 가장 먼저 깨달은 것은 분산 락의 세밀도가 성능에 얼마나 큰 영향을 미치는가 하는 점이었다. 락의 범위가 넓을수록 안전하지만 성능은 떨어진다. 전역 락은 매우 안전하지만 동시 처리가 불가능하고, 사용자별 락은 약간 덜 안전하지만 동시 처리가 가능하며, 상품별 락은 더 세밀하지만 데드락 위험이 있고, 락이 아예 없으면 성능은 최고지만 안전성은 최악이다.

이러한 트레이드오프에서 최선의 선택을 하려면 비즈니스 특성을 고려해야 한다. 우리 시스템의 경우 서로 다른 사용자의 주문은 독립적이므로 사용자별 락이 적절했다. 하지만 동일 사용자의 중복 주문은 막아야 하고, 재고 동시성은 데이터베이스 레벨에서 처리해야 한다. 이렇게 여러 계층에서 검증하는 다층 방어 전략을 통해 성능과 안전성을 모두 확보할 수 있었다.

두 번째로 깨달은 것은 다층 방어의 중요성이었다. 단일 안전장치에만 의존하면 그것이 실패했을 때 시스템 전체가 위험해진다. 예를 들어 분산 락만 의존하면 Redis 장애 시 동시성 제어가 완전히 실패하여 재고가 음수로 갈 수 있다. 하지만 다층 방어 전략을 사용하면 첫 번째 계층인 분산 락이 실패하더라도 두 번째 계층인 데이터베이스 쿼리 검증이 있고, 그것도 실패하면 세 번째 계층인 트랜잭션 롤백이 있다. 하나의 안전장치에만 의존하지 않고 여러 계층에서 검증하는 것이 훨씬 안전하다.

세 번째로 충격적이었던 것은 인덱스의 위력이었다. 인덱스 하나를 추가하는 단순한 작업으로 쿼리 시간을 60ms에서 1ms로 60배 개선할 수 있었고, 이것이 전체 TPS를 43에서 78로 1.8배 향상시켰다. 복잡한 알고리즘 최적화나 캐싱 레이어 도입보다 훨씬 간단한 작업으로 큰 효과를 얻은 것이다. 이를 통해 성능 최적화는 항상 측정부터 시작해야 한다는 것을 배웠다. 추측으로 복잡한 최적화를 시도하기보다, 로그 분석이나 EXPLAIN 실행 계획, Slow Query Log 등을 통해 실제 병목을 찾아내는 것이 훨씬 효과적이다.

네 번째로 깨달은 것은 로컬 환경의 한계였다. 현재 달성한 TPS 81.68은 로컬 MySQL 환경에서 나온 수치다. 로컬 환경은 SSD 디스크를 사용하고 단일 서버에서 복제 없이 운영되므로, 이 환경에서 78 TPS는 매우 훌륭한 수치다. 하지만 프로덕션 MySQL 환경이라면 RAID 구성, Read Replica 3대, Connection Pool 튜닝 등을 통해 300~500 TPS도 충분히 가능할 것으로 예상된다. 따라서 로컬 테스트 결과를 프로덕션 성능으로 직접 환산하기보다는, 상대적인 개선율(10.8배)에 더 주목해야 한다.

### 6.2 개발 프로세스 개선

이번 프로젝트를 진행하면서 나는 성능 최적화에 대한 접근 방식을 완전히 바꾸게 되었다. 처음에는 추측 기반으로 최적화를 시도했다. 전역 락이 문제일 것 같아서 코드를 변경하고, 효과가 미미하니 Redis가 느린가 의심하여 Redis 로직을 주석 처리하고, 그래도 안 되니 Outbox가 느린가 의심하여 Outbox 로직도 주석 처리하는 식이었다. 이러한 방식은 근거 없는 추측에 기반하고 시행착오를 반복하며, 실제 병목을 놓칠 위험이 있었다.

하지만 측정 기반 접근 방식으로 전환한 후 모든 것이 달라졌다. 먼저 TraceAspect를 통해 로그를 수집하고, 각 메서드의 실행 시간을 측정했다. 그 결과 CartItemRepository가 60ms로 전체의 88%를 차지한다는 것을 발견했다. 그 다음 EXPLAIN으로 쿼리 실행 계획을 분석하여 인덱스가 없음을 확인했다. 그리고 인덱스를 추가한 후 다시 측정하여 효과를 즉시 확인했다. 이러한 방식은 데이터 기반 의사결정이 가능하고, 우선순위가 명확하며, 효과를 즉시 확인할 수 있다는 장점이 있었다.

측정하지 않으면 개선할 수 없다. 추측으로는 최적화를 할 수 없고, 반드시 데이터에 기반해야 한다. 또한 한 번에 여러 가지를 변경하면 무엇이 효과가 있었는지 알 수 없으므로, 한 번에 하나씩 변경하고 측정해야 한다. 이러한 원칙들이 효과적인 성능 최적화의 핵심이라는 것을 깨달았다.

### 6.3 아키텍처 설계 원칙

이번 프로젝트를 통해 나는 YAGNI(You Aren't Gonna Need It)와 확장성 사이의 균형에 대해서도 생각해보게 되었다. 초기 설계에서 전역 락을 사용한 것은 YAGNI 원칙에 따른 것이었다. 단순하고 안전한 구현으로 시작하고, 복잡한 데드락 회피 로직은 나중에 필요하면 추가하자는 생각이었다. 이 접근 자체는 나쁘지 않았지만, 성능은 처음부터 고려했어야 했다.

나는 다음과 같은 균형점을 찾았다. 일단 간단한 구현으로 시작하되, 측정 가능하게 만들어야 한다. 로깅과 모니터링을 처음부터 추가하여 성능 문제를 조기에 발견할 수 있도록 한다. 그리고 병목이 발견되면 즉시 최적화한다. 이렇게 하면 과도한 조기 최적화를 피하면서도, 실제 문제가 발생했을 때 빠르게 대응할 수 있다.

또한 설계 결정의 이유를 코드에 남기는 것이 얼마나 중요한지도 깨달았다. 현재 프로젝트의 좋은 예로, CreateOrderUseCase에는 전역 락을 사용하는 이유가 주석으로 명시되어 있었다. 주문은 여러 상품의 재고를 동시에 차감하므로 각 상품에 개별 락을 걸면 데드락 위험이 있어서 단순하고 안전한 전역 락 방식을 채택했다는 설명이었다. 이 주석 덕분에 나는 초기 설계자의 의도를 이해할 수 있었고, 그것을 고려하여 더 나은 해결책(사용자별 락 + DB 레벨 검증)을 찾을 수 있었다. 설계 결정의 이유를 코드에 남기면 미래의 자신이나 다른 개발자가 고마워할 것이다.

점진적 개선의 중요성도 배웠다. 나는 한 번에 모든 것을 바꾸지 않고 단계적으로 접근했다. 먼저 사용자별 락을 적용하여 TPS를 3.4배 향상시키고 안전성을 검증했다. 그 다음 커넥션 풀을 확대하여 추가 향상을 얻었고, 마지막으로 인덱스를 추가하여 최종적으로 10.8배 달성했다. 각 단계마다 측정하고 검증했기 때문에, 만약 문제가 발생하더라도 어느 단계에서 문제가 생겼는지 쉽게 파악할 수 있었다. 작은 변경을 반복하며 측정하는 것이 한 번에 여러 변경을 하는 것보다 훨씬 안전하고 효과적이다.

### 6.4 성능 테스트 방법론

k6를 사용한 부하 테스트 경험도 매우 유익했다. 실제 시나리오를 재현하여 장바구니 추가부터 주문 완료까지의 전 과정을 테스트했고, TPS, 응답시간, 에러율 등 다양한 메트릭을 수집했으며, ramping-vus 방식으로 점진적으로 부하를 증가시켜 시스템의 한계를 찾았다. 이러한 방식은 실제 프로덕션 환경을 잘 시뮬레이션할 수 있었다.

하지만 개선할 점도 발견했다. sleep 시간 조정이 필요했다. 초기에는 실사용자를 시뮬레이션하기 위해 2~5초의 sleep을 넣었지만, 순수 시스템 성능을 측정할 때는 sleep을 제거하는 것이 더 정확했다. 또한 시나리오를 다양화할 필요가 있었다. 쿠폰 사용률을 조정하거나 재고 소진 시나리오를 추가하면 더 현실적인 테스트가 가능할 것이다.

로그 기반 프로파일링도 매우 유용했다. ThreadLocalLogTrace를 사용하여 전체 호출 스택을 시각화하고 각 단계별 시간을 측정할 수 있었다. 이를 통해 병목 지점을 즉시 파악할 수 있었다. 좋은 로깅은 최고의 디버깅 도구다. 로그를 잘 설계하면 복잡한 프로파일링 도구 없이도 성능 문제를 쉽게 찾아낼 수 있다.

### 6.5 실무 적용 시 고려사항

이러한 최적화 작업을 실제 프로덕션 환경에 적용할 때는 추가 고려사항이 있다. 먼저 모니터링이 필수적이다. 비즈니스 메트릭으로는 주문 TPS, 주문 성공률, 평균 주문 금액을 추적해야 하고, 시스템 메트릭으로는 API 응답 시간(p50, p95, p99), DB Connection Pool 사용률, Redis 락 대기 시간, 에러율(재고 부족, 잔액 부족) 등을 모니터링해야 한다. 인프라 메트릭으로는 CPU 사용률, 메모리 사용률, DB 슬로우 쿼리 등을 확인해야 한다.

이러한 메트릭을 수집하기 위해서는 적절한 도구가 필요하다. APM은 Datadog, New Relic, Pinpoint 같은 도구를 사용할 수 있고, 메트릭 수집은 Prometheus와 Grafana를 조합하면 좋으며, 로그 분석은 ELK Stack(Elasticsearch, Logstash, Kibana)이 유용하다.

알림 설정도 중요하다. TPS가 50 미만으로 떨어지면 즉시 알림을 보내야 하고, P95 응답시간이 500ms를 초과하면 5분 내에 조치해야 하며, 에러율이 5%를 넘으면 즉시 알림을 받아야 한다. 이러한 임계값은 시스템 특성에 따라 조정할 수 있다.

부하 테스트는 자동화하는 것이 좋다. CI/CD 파이프라인에 통합하여 Pull Request마다 자동으로 부하 테스트를 실행하고, TPS가 기준치 이하로 떨어지면 배포를 차단하는 식으로 성능 회귀를 방지할 수 있다.

마지막으로 카나리 배포를 통해 점진적으로 롤아웃하는 것이 안전하다. 먼저 5%의 트래픽으로 1시간 동안 모니터링하고, 이상이 없으면 25%로 늘려서 1시간 더 지켜보고, 다시 이상이 없으면 50%로 늘린 후 최종적으로 100%로 전환한다. 이렇게 하면 문제가 발생하더라도 영향 범위를 최소화할 수 있다.

---

## 7. 결론

### 7.1 성과 요약

이번 성능 최적화 프로젝트를 통해 나는 TPS를 7.54에서 81.68로 10.8배 향상시켰다. 동시 처리 능력은 1개에서 100개로 100배 증가했고, 일일 처리량은 162,864건에서 1,764,288건으로 늘어났다. 서버 비용은 90% 절감할 수 있게 되었고(13대에서 1.3대로), 에러율은 0.12%에서 0.00%로 완전히 제거되었다.

이러한 정량적 성과도 중요하지만, 정성적 성과도 의미가 있다. 시스템이 실사용 가능한 수준으로 성능이 개선되었고, 사용자별 락과 DB 검증을 통해 데이터 정합성을 유지했으며, 확장 가능한 아키텍처를 구축했고, 측정 기반 최적화 문화를 확립했다.

특히 강조하고 싶은 것은 이 모든 성과가 실제 운영에 필요한 기능을 모두 포함한 상태에서 달성되었다는 점이다. Redis 랭킹 업데이트, Outbox 이벤트 저장, 쿠폰 사용 처리, 결제 히스토리 기록 등 비즈니스에 필수적인 모든 기능이 활성화된 상태에서 TPS 81.68을 달성했다. 이는 단순한 벤치마크가 아니라 실제 프로덕션 환경에서 기대할 수 있는 성능이다.

### 7.2 핵심 교훈

이번 프로젝트를 통해 나는 몇 가지 중요한 교훈을 얻었다.

첫째, 측정하지 않으면 개선할 수 없다는 것을 배웠다. 추측으로는 최적화를 할 수 없고, 반드시 실제 데이터에 기반해야 한다. TraceAspect 로그를 분석하고, EXPLAIN으로 쿼리 실행 계획을 확인하고, Redis 락을 모니터링하는 등의 측정 작업이 없었다면 실제 병목을 찾지 못했을 것이다.

둘째, 추측보다는 측정이 우선이라는 것을 깨달았다. Redis가 느릴 것 같다거나 Outbox가 문제일 것 같다는 추측으로 코드를 수정하는 대신, 실제로 어느 부분이 60ms를 소비하는지 측정하여 정확한 병목을 찾아냈다. 이 과정에서 시행착오를 크게 줄일 수 있었다.

셋째, 한 번에 하나씩 변경하고 측정하는 것이 중요하다는 것을 배웠다. 분산 락, 커넥션 풀, 인덱스를 각각 단계적으로 적용하고 매번 성능을 측정했기 때문에, 각 변경사항의 효과를 정확히 파악할 수 있었다. 만약 모든 것을 동시에 바꿨다면 무엇이 효과가 있었는지 알 수 없었을 것이다.

넷째, 안전장치는 여러 겹으로 구성해야 한다는 것을 깨달았다. 분산 락만 의존하지 않고 DB 레벨 검증을 추가했고, 그것도 실패하면 트랜잭션 롤백이 있다. 단일 실패 지점에 의존하지 않고 여러 계층에서 방어하는 것이 훨씬 안전하다.

다섯째, 설계 결정을 문서화하는 것이 미래의 자신에게 주는 선물이라는 것을 배웠다. 전역 락을 사용한 이유가 주석으로 명시되어 있었기 때문에, 나는 그것을 이해하고 더 나은 해결책을 찾을 수 있었다. 코드에 '무엇을' 하는지뿐만 아니라 '왜' 그렇게 하는지를 남기는 것이 중요하다.

### 7.3 향후 개선 방향

현재 시스템은 일반적인 트래픽을 충분히 처리할 수 있지만, 더 나은 성능을 위해 추가 개선을 고려할 수 있다.

단기적으로는 읽기 성능 최적화에 집중할 수 있다. Read Replica를 도입하여 조회 쿼리의 부하를 분산시키고, 상품 정보를 Redis에 캐싱하여 데이터베이스 부하를 줄이며, 장바구니 조회를 최적화할 수 있다. 또한 모니터링을 강화하여 APM 도구를 도입하고, 알림을 설정하며, 대시보드를 구축할 수 있다.

중기적으로는 CQRS 패턴 적용을 고려할 수 있다. 쓰기 작업(Command)인 주문 생성과 재고 차감은 현재대로 메인 데이터베이스에서 처리하고, 읽기 작업(Query)인 주문 조회와 인기 상품 조회는 별도의 읽기 전용 데이터베이스나 캐시를 사용하는 방식이다. 또한 비동기 처리를 확대하여 결제를 Saga 패턴으로 비동기화하고, 이메일 발송과 푸시 알림도 비동기화할 수 있다.

장기적으로는 대규모 트래픽에 대응하기 위한 샤딩 전략을 고려할 수 있다. 사용자 ID 기반으로 데이터베이스를 4개의 샤드로 분할하여 수평 확장을 가능하게 한다. 또한 이벤트 소싱을 도입하여 주문 상태를 직접 저장하는 대신 주문 관련 모든 이벤트를 저장하면, 완벽한 감사 로그를 남길 수 있고 시점별 상태를 재구성할 수도 있다.

---

## 8. 부록

### 8.1 전체 변경 사항 요약

이번 최적화 작업에서 수정한 파일들을 정리하면 다음과 같다.

| 파일 | 변경 내용 | 영향도 |
|------|-----------|--------|
| `CreateOrderUseCase.java` | 전역 락 → 사용자별 락 | 높음 |
| `ProductRepository.java` | 재고 차감 쿼리 개선 | 중간 |
| `OrderFacade.java` | 재고 검증 로직 추가 | 중간 |
| `CartItem.java` | cart_id 인덱스 추가 | 높음 |
| `application.yml` | Connection Pool 설정 | 낮음 |
| `order-payment-test.js` | sleep 제거, VU 증가 | 낮음 |

각 파일의 주요 변경 내용을 코드로 보면 다음과 같다.

CreateOrderUseCase.java에서는 분산 락 키를 전역에서 사용자별로 변경했다.

```diff
- @DistributedLock(key = "'order:global'")
+ @DistributedLock(key = "'order:user:' + #request.userId")
```

ProductRepository.java에서는 재고 차감 쿼리에 WHERE 조건을 추가하여 재고가 충분할 때만 차감되도록 했다.

```diff
- @Query("UPDATE Product p SET p.stock.quantity = p.stock.quantity - :amount WHERE p.productId = :productId")
+ @Query("UPDATE Product p SET p.stock.quantity = p.stock.quantity - :amount WHERE p.productId = :productId AND p.stock.quantity >= :amount")
```

OrderFacade.java에서는 UPDATE 결과를 확인하여 재고 부족 시 예외를 던지도록 했다.

```diff
  private void deductStock(...) {
      for (...) {
-         productRepository.decreaseStock(productId, quantity);
+         int updated = productRepository.decreaseStock(productId, quantity);
+         if (updated == 0) {
+             throw new InsufficientStockException("재고 부족");
+         }
      }
  }
```

CartItem.java에서는 테이블 정의에 인덱스를 추가했다.

```diff
  @Entity
- @Table(name = "cart_items")
+ @Table(name = "cart_items", indexes = {
+     @Index(name = "idx_cart_id", columnList = "cart_id")
+ })
  public class CartItem {
```

application.yml에서는 HikariCP 설정을 명시적으로 추가했다.

```diff
  spring:
    datasource:
      url: jdbc:mysql://localhost:3306/ecommerce
+     hikari:
+       maximum-pool-size: 100
+       minimum-idle: 20
```

order-payment-test.js에서는 불필요한 sleep을 제거했다.

```diff
  export default function () {
      const orderResponse = http.post(...);
-     sleep(Math.random() * 3 + 2);
+     // sleep 제거
  }
```

### 8.2 성능 테스트 체크리스트

다음번에 성능 테스트를 진행할 때 참고할 수 있도록 체크리스트를 정리했다.

테스트 전 준비 단계에서는 LoadTestDataSeeder를 실행하여 테스트 데이터를 준비하고, 애플리케이션을 재시작하며, Redis와 MySQL이 정상 실행 중인지 확인하고, 로그 레벨이 적절히 설정되어 있는지 확인해야 한다.

테스트 실행 단계에서는 k6가 설치되어 있는지 확인하고, 테스트 스크립트가 올바른지 검증하며, 변경 전 베이스라인을 측정한 후 변경을 적용하고 다시 측정해야 한다.

결과 분석 단계에서는 TPS를 비교하고, 응답 시간을 평균, p95, p99로 비교하며, 에러율을 확인하고, 로그를 분석하여 병목 지점을 찾으며, Redis 락 상태를 확인해야 한다.

### 8.3 참고 자료

이번 프로젝트를 진행하면서 참고한 자료들을 정리했다. 분산 락과 관련해서는 Redisson Documentation, Redis Distributed Locks 문서, Martin Kleppmann의 "How to do distributed locking" 글이 유용했다. 데이터베이스 최적화와 관련해서는 MySQL Performance Tuning 공식 문서, "Use The Index, Luke" 웹사이트, "High Performance MySQL" 책이 도움이 되었다. 부하 테스트는 k6 Documentation과 Load Testing Best Practices 문서를 참고했고, 아키텍처 패턴으로는 Outbox Pattern, CQRS, Event Sourcing에 대한 Martin Fowler의 글들이 유익했다.

---

## 맺음말

이번 성능 최적화 프로젝트를 통해 나는 측정 기반의 체계적인 접근이 얼마나 중요한지 깊이 깨달았다. 단순히 코드를 잘 작성하는 것만으로는 충분하지 않다. 시스템의 동작을 관찰하고, 데이터를 수집하고, 그것을 분석하여 실제 문제를 찾아내는 능력이 필요하다.

추측이 아닌 데이터로 판단하고, 하나가 아닌 여러 계층의 안전장치를 구축하며, 한 번에가 아닌 점진적으로 개선하는 것. 이것이 안정적이면서도 효과적인 성능 최적화의 핵심이었다.

TPS를 10.8배 향상시켰다는 정량적 성과도 의미가 있지만, 그보다 더 중요한 것은 이 과정에서 배운 방법론과 사고방식이다. 어떻게 문제를 찾고, 어떻게 가설을 세우고, 어떻게 검증하며, 어떻게 개선하는가. 이러한 경험은 앞으로 마주칠 다양한 성능 문제를 해결하는 데 큰 자산이 될 것이다.

성능 최적화는 끝이 없는 여정이다. 현재 81.68 TPS도 충분히 좋은 성능이지만, 앞으로 트래픽이 더 증가하면 추가 최적화가 필요할 것이다. 그때도 이번에 배운 측정 기반 접근 방식을 적용하여, 추측이 아닌 데이터로, 점진적이고 안전하게 개선해나갈 것이다.

---

_이 문서는 실제 성능 최적화 프로젝트의 전 과정을 상세히 기록한 것입니다._
